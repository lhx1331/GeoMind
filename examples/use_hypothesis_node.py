"""
Hypothesis èŠ‚ç‚¹ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Hypothesis èŠ‚ç‚¹æ ¹æ®çº¿ç´¢ç”Ÿæˆåœ°ç†å‡è®¾ã€‚
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from geomind.agent.nodes.hypothesis import (
    create_clues_summary,
    hypothesis_node,
    hypothesis_node_iterative,
    hypothesis_node_with_validation,
)
from geomind.agent.state import AgentState, Clues, Metadata, OCRText, VisualFeature


async def example_1_basic_usage():
    """ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: Hypothesis èŠ‚ç‚¹åŸºç¡€ä½¿ç”¨")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•çº¿ç´¢ï¼ˆæ¨¡æ‹Ÿ Perception é˜¶æ®µçš„è¾“å‡ºï¼‰
    clues = Clues(
        ocr=[
            OCRText(text="Shibuya Crossing", bbox=[100, 200, 300, 250], confidence=0.95),
            OCRText(text="æ¸‹è°·", bbox=[100, 260, 300, 310], confidence=0.90),
            OCRText(text="Tokyo", bbox=[310, 200, 400, 250], confidence=0.85),
        ],
        visual=[
            VisualFeature(type="landmark", value="busy intersection", confidence=0.90),
            VisualFeature(type="urban", value="modern city", confidence=0.85),
            VisualFeature(type="signage", value="Japanese characters", confidence=0.88),
        ],
        meta=Metadata(
            scene_type="urban",
            time_of_day="day",
        ),
    )
    
    # åˆ›å»º Agent çŠ¶æ€
    state = AgentState(
        image_path="test_image.jpg",
        clues=clues,
    )
    
    print(f"\nè¾“å…¥çº¿ç´¢:")
    print(f"  OCR æ–‡æœ¬: {len(clues.ocr)} ä¸ª")
    for ocr in clues.ocr:
        print(f"    - {ocr.text} (ç½®ä¿¡åº¦: {ocr.confidence:.2f})")
    
    print(f"\n  è§†è§‰ç‰¹å¾: {len(clues.visual)} ä¸ª")
    for vf in clues.visual:
        print(f"    - {vf.type}: {vf.value} (ç½®ä¿¡åº¦: {vf.confidence:.2f})")
    
    # æ‰§è¡Œ Hypothesis èŠ‚ç‚¹
    print(f"\næ‰§è¡Œ Hypothesis èŠ‚ç‚¹...")
    result = await hypothesis_node(state)
    
    # æŸ¥çœ‹ç»“æœ
    hypotheses = result["hypotheses"]
    
    print(f"\nâœ… Hypothesis å®Œæˆï¼")
    print(f"\nç”Ÿæˆçš„åœ°ç†å‡è®¾: {len(hypotheses)} ä¸ª")
    
    for i, h in enumerate(hypotheses, 1):
        print(f"\nå‡è®¾ {i}:")
        print(f"  åŒºåŸŸ: {h.region}")
        print(f"  ç½®ä¿¡åº¦: {h.confidence:.2f}")
        print(f"  æ¨ç†: {h.rationale}")
        print(f"  æ”¯æŒè¯æ®: {', '.join(h.supporting) if h.supporting else 'æ— '}")
        print(f"  å†²çªè¯æ®: {', '.join(h.conflicting) if h.conflicting else 'æ— '}")


async def example_2_clues_summary():
    """ç¤ºä¾‹ 2: çº¿ç´¢æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2: åˆ›å»ºçº¿ç´¢æ‘˜è¦")
    print("=" * 60)
    
    # åˆ›å»ºä¸°å¯Œçš„çº¿ç´¢
    clues = Clues(
        ocr=[
            OCRText(text="Big Ben", bbox=[50, 100, 200, 150], confidence=0.98),
            OCRText(text="Westminster", bbox=[50, 160, 200, 200], confidence=0.92),
        ],
        visual=[
            VisualFeature(type="landmark", value="clock tower", confidence=0.95),
            VisualFeature(type="architecture", value="Gothic style", confidence=0.88),
        ],
        meta=Metadata(
            gps={"GPSLatitude": 51.5007, "GPSLongitude": -0.1246},
            timestamp="2024:06:15 14:30:00",
            camera_info="iPhone 15 Pro",
        ),
    )
    
    # åˆ›å»ºæ‘˜è¦
    summary = create_clues_summary(clues)
    
    print(f"\nçº¿ç´¢æ‘˜è¦:")
    print("-" * 60)
    print(summary)
    print("-" * 60)
    
    print(f"\nğŸ“ è¿™ä¸ªæ‘˜è¦å°†è¢«å‘é€ç»™ LLM è¿›è¡Œå‡è®¾ç”Ÿæˆ")


async def example_3_with_validation():
    """ç¤ºä¾‹ 3: ä½¿ç”¨éªŒè¯è¿‡æ»¤"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3: ä½¿ç”¨éªŒè¯è¿‡æ»¤ä½ç½®ä¿¡åº¦å‡è®¾")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•çº¿ç´¢
    clues = Clues(
        ocr=[
            OCRText(text="Eiffel Tower", bbox=[100, 200, 300, 250], confidence=0.95),
        ],
        visual=[
            VisualFeature(type="landmark", value="tower", confidence=0.90),
        ],
        meta=Metadata(),
    )
    
    state = AgentState(image_path="test.jpg", clues=clues)
    
    print(f"\næ‰§è¡Œå¸¦éªŒè¯çš„ Hypothesis èŠ‚ç‚¹...")
    print(f"  è®¾ç½®æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼: 0.5")
    
    # ä½¿ç”¨éªŒè¯
    result = await hypothesis_node_with_validation(
        state,
        min_confidence=0.5,  # è¿‡æ»¤ä½äº 0.5 çš„å‡è®¾
    )
    
    hypotheses = result["hypotheses"]
    
    print(f"\nâœ… ç”Ÿæˆå¹¶è¿‡æ»¤åçš„å‡è®¾: {len(hypotheses)} ä¸ª")
    
    for i, h in enumerate(hypotheses, 1):
        print(f"\nå‡è®¾ {i}:")
        print(f"  åŒºåŸŸ: {h.region}")
        print(f"  ç½®ä¿¡åº¦: {h.confidence:.2f} âœ“ (â‰¥ 0.5)")


async def example_4_iterative():
    """ç¤ºä¾‹ 4: è¿­ä»£ä¼˜åŒ–å‡è®¾"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4: è¿­ä»£å¼å‡è®¾ç”Ÿæˆ")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•çº¿ç´¢
    clues = Clues(
        ocr=[
            OCRText(text="Colosseum", bbox=[100, 200, 300, 250], confidence=0.96),
        ],
        visual=[
            VisualFeature(type="landmark", value="ancient amphitheater", confidence=0.92),
        ],
        meta=Metadata(),
    )
    
    state = AgentState(image_path="test.jpg", clues=clues)
    
    print(f"\næ‰§è¡Œè¿­ä»£å¼ Hypothesis èŠ‚ç‚¹...")
    print(f"  è¿­ä»£æ¬¡æ•°: 2")
    print(f"  æ¯æ¬¡è¿­ä»£åŸºäºä¸Šæ¬¡ç»“æœè¿›è¡Œä¼˜åŒ–")
    
    # ä½¿ç”¨è¿­ä»£æ¨¡å¼
    result = await hypothesis_node_iterative(
        state,
        max_iterations=2,
    )
    
    hypotheses = result["hypotheses"]
    
    print(f"\nâœ… è¿­ä»£ä¼˜åŒ–å®Œæˆ")
    print(f"\næœ€ç»ˆå‡è®¾: {len(hypotheses)} ä¸ª")
    
    for i, h in enumerate(hypotheses, 1):
        print(f"\nå‡è®¾ {i}:")
        print(f"  åŒºåŸŸ: {h.region}")
        print(f"  ç½®ä¿¡åº¦: {h.confidence:.2f}")


async def example_5_in_phrv_workflow():
    """ç¤ºä¾‹ 5: åœ¨å®Œæ•´ PHRV å·¥ä½œæµä¸­ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 5: åœ¨å®Œæ•´ PHRV å·¥ä½œæµä¸­ä½¿ç”¨")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–çŠ¶æ€ï¼ˆå‡è®¾å·²ç»è¿‡ Perception é˜¶æ®µï¼‰
    clues = Clues(
        ocr=[
            OCRText(text="Sydney Opera House", bbox=[100, 200, 400, 250], confidence=0.97),
        ],
        visual=[
            VisualFeature(type="landmark", value="distinctive shell roof", confidence=0.94),
            VisualFeature(type="location", value="waterfront", confidence=0.89),
        ],
        meta=Metadata(
            scene_type="outdoor",
            time_of_day="day",
        ),
    )
    
    state = AgentState(
        image_path="test_sydney.jpg",
        clues=clues,
    )
    
    print(f"\nğŸ“ PHRV æµç¨‹:")
    print(f"  [1/4] P (Perception)  âœ“ å·²å®Œæˆ")
    print(f"  [2/4] H (Hypothesis)  â† å½“å‰é˜¶æ®µ")
    print(f"  [3/4] R (Retrieval)")
    print(f"  [4/4] V (Verification)")
    
    # 2. æ‰§è¡Œ Hypothesis é˜¶æ®µ
    print(f"\nğŸ” æ‰§è¡Œ Hypothesis é˜¶æ®µ...")
    hypothesis_result = await hypothesis_node(state)
    
    # 3. æ›´æ–°çŠ¶æ€
    state.hypotheses = hypothesis_result["hypotheses"]
    
    print(f"\nâœ… Hypothesis é˜¶æ®µå®Œæˆ")
    print(f"\nçŠ¶æ€æ›´æ–°:")
    print(f"  çº¿ç´¢: {len(state.clues.ocr)} OCR + {len(state.clues.visual)} è§†è§‰ç‰¹å¾")
    print(f"  å‡è®¾: {len(state.hypotheses)} ä¸ª")
    
    for i, h in enumerate(state.hypotheses[:3], 1):
        print(f"    {i}. {h.region} (ç½®ä¿¡åº¦: {h.confidence:.2f})")
    
    print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
    print(f"  â†’ è¿›å…¥ Retrieval é˜¶æ®µ")
    print(f"  â†’ ä½¿ç”¨ GeoCLIP ä¸ºæ¯ä¸ªå‡è®¾å¬å›å€™é€‰åœ°ç‚¹")


async def example_6_with_custom_llm():
    """ç¤ºä¾‹ 6: ä½¿ç”¨è‡ªå®šä¹‰ LLM æä¾›å•†"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 6: ä½¿ç”¨è‡ªå®šä¹‰ LLM æä¾›å•†")
    print("=" * 60)
    
    clues = Clues(
        ocr=[OCRText(text="Test", bbox=[0, 0, 100, 100], confidence=0.9)],
        visual=[],
        meta=Metadata(),
    )
    
    state = AgentState(image_path="test.jpg", clues=clues)
    
    # æ”¯æŒçš„ LLM æä¾›å•†
    llm_providers = ["openai", "anthropic", "deepseek"]
    
    print(f"\næ”¯æŒçš„ LLM æä¾›å•†:")
    for provider in llm_providers:
        print(f"  - {provider}")
    
    # ä½¿ç”¨ DeepSeekï¼ˆç¤ºä¾‹ï¼‰
    try:
        print(f"\nä½¿ç”¨ LLM æä¾›å•†: deepseek")
        result = await hypothesis_node(state, llm_provider="deepseek")
        
        print(f"\nâœ… ä½¿ç”¨è‡ªå®šä¹‰ LLM å®Œæˆ")
        print(f"  ç”Ÿæˆå‡è®¾: {len(result['hypotheses'])} ä¸ª")
        
    except Exception as e:
        print(f"\nâš ï¸ è‡ªå®šä¹‰ LLM å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦ API Keyï¼‰: {e}")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\nğŸš€ GeoMind Hypothesis èŠ‚ç‚¹ä½¿ç”¨ç¤ºä¾‹\n")
    
    try:
        # ç¤ºä¾‹ 1
        await example_1_basic_usage()
        
        # ç¤ºä¾‹ 2
        await example_2_clues_summary()
        
        # ç¤ºä¾‹ 3
        await example_3_with_validation()
        
        # ç¤ºä¾‹ 4
        await example_4_iterative()
        
        # ç¤ºä¾‹ 5
        await example_5_in_phrv_workflow()
        
        # ç¤ºä¾‹ 6
        await example_6_with_custom_llm()
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
    
    print("\nğŸ“š ç›¸å…³æ–‡æ¡£:")
    print("  - geomind/agent/nodes/hypothesis.py")
    print("  - tests/unit/test_hypothesis_node.py")
    print("  - geomind/prompts/hypothesis.py")


if __name__ == "__main__":
    asyncio.run(main())

